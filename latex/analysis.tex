A full factorial $2^k$ experiment with $k = 4$ factors is conducted, with the order of the individual runs randomly generated by \texttt{FrF2}. The number of individual runs is $n = 16$. The number of covariates, $p$, will depend on how many of the interaction effects one wishes to include as covariates. Replicates of the individual runs is not necessary due to the way the response is found. The response is already the smallest time recorded when calling a function 100 times in a row. The probability that the response has taken on an very unusual value is therefore already quite low. Each run is not necessarily an genuine run replicate since not all processes taking up resources on the computer and be monitored and controlled. Despite this the response is not expected to have been significantly impacted by this. This is due the fact that the function is called several times before the response is recorded, and the processes would probably not run for the full duration of the test. 


The first step of the analysis involves fitting the full model. This produces estimates for $\mathbf{\beta}$, but the standard error of the individual ${\beta}_k$ can not be computed due to the degrees of freedom, $n - p$, being zero. Because of this the significant| covariates can not be found in the traditional way. An alternative approach must therefore be taken. Lenth provided a method for estimating which covariates are significant. This method is implemented in the code, see the function \texttt{get_lenth}. Lenth's approach reveals that the four factor interaction isn't significant for any reasonable significance level. This, in addition with the unclear meaning of such an interaction leads us to drop the four factor interaction from the model in order to obtain a degree of freedom. 

Fitting a new, reduced model without the four factor interaction reveals that none of three factor interactions are significant either. One can therefore proceed with a model with only one - and two - factor interactions. This has the benefit of enabling us to make plots of the residuals in order to evaluate the correctness of the assumptions made when constructing a linear model. 

Before one proceeds to constructing the final model it is sensible to consider how a transformation of the response could impact the fit of the model. A widely used transformation is the Box-Cox transformation. The value that maximizes the log-likelihood function calculated by \texttt{boxcox} is chosen as $\lambda$ is the transformation given by the following equation. $T_{new}$ denotes the response. $T$ denotes the new response.

\begin{equation}
T_{new} = \frac{T^{\lambda} - 1}{\lambda}
\end{equation}

This procedure is carried out in the code. Lambda is found by the use of \texttt{get_lambda}.

The first

A new model with the transformed data is then fitted. Calling \texttt{summary} on the model, as done in the code, returns a overview of the most important properties of the model. $R^2$ is very high, but this is in part caused by the large number of covariates. This is indicated by $R^2_{adj}$. 



