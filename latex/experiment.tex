The insertion sort algorithm is conceptually quite simple. When implementing it, the programmer does not have a lot of choices regarding the outline of the algorithm, but he/she does have a lot of choices regarding how to specifically apply the programming language and its features. We have chosen to investigate the effect of 4 such choices (a.k.a. factors) on the runtime of the algorithm.    

\begin{itemize}
    \item Factor A: How to implement an interchange of values between two variables. Here you have the choice between creating a temporary variable in order to store the value of the first variable, while you change it to the value of the second one,
    \begin{lstlisting}[language=Python]
        temp = array[j-1]
        array[j-1] = array[j]
        array[j] = temp
    \end{lstlisting}
    versus using the more idiomatic aproach (often called `pythonic') of tuple reassignment, as shown here,
    \begin{lstlisting}[language=Python]
        array[j-1], array[j] = array[j], array[j-1]
    \end{lstlisting}
    When $A=-1$, a temporary variable is used, and when $A=1$, tuple reassignment is used.
 
    \item Factor B: How to create an iteration variable in order to keep track of how much of the array that has already been sorted. There are two ways to do this in Python 2.7, which we will use during the experiment. The first one is to generate a list with the \texttt{range()} function and then iterate over it. The second one is to initialize a so-called `generator' which uses far less memory since it only keeps track of one single value at a time, incrementing it when required. In general, the decrease in memory use often comes at the cost of CPU-time, so the generator must not necessarily be a faster construct. When $B=-1$, a list is used, and when $B=1$, a generator is used.
    
    \item Factor C: The numbers which will be sorted by the different algorithm implementations are placed within the `list' data type in Python. This is a mutable data type, so the numbers can be sorted in place. The question therefore arises if it is beneficial to return the sorted list from the sort function, or to just mutate the list in place. A so-called `functional' programming style calls for returning the list as it is more explicit, but we hypotesize that this return, and the consequential initialization of a new array variable comes at a small cost. When $C=-1$, the functional style is used, while $C=1$ indicates that the values are mutated in place without a return statement.
    
    \item Factor D: All of the randomly generated numbers to be sorted are integers. They are chosen to have random values between $\pm$\texttt{os.maxsize}, the maximum absolute values which the Python native integer type can handle. When reading these numbers into the list, one has to know a priori if all of the numbers can be precisely defined as integers or not. This is not always the case. Therefore we investigate the effect of sorting the entire list interpreted as integers versus interpreting them as floating points. $D=-1$ when the numbers are sorted as integer data types, while $D=1$ when the numdbers are interpreted in the more general floating point data type.
\end{itemize}

The first 3 factors are related directly to how the sorting algorithm is implemented. This results in 8 different implementations, all are written in the module \texttt{sorting{\_}algorithms.py}. The python code and the subsequent analysis in R can be found at \url{https://github.com/JakobGM/python-runtime-testing}. We could have written only one function which not only just takes in a list of numbers to sort, but also the values of the factors $A$, $B$, and $C$, and then use the corresponding techniques. We chose to not do it this way, since the evaluation of these factors in the function body could probably impact the runtime of the function itself. Therefore it is better to write 8 different implementations, even if it goes against the DRY principle of programming, "Don't Repeat Yourself".

The last $D$ factor is related to how the numbers which are to be sorted are interpreted. A module called \texttt{generate{\_}random.py} has been written in order to generate a set of random numbers. In our case, we generated 10 000 such numbers. These are then written to a \texttt{.csv} file, such that the same list of numbers is used for all of the 16 runs, reducing the variability of the experiments and contributing to making each run closer to a genuine replicate.

In order to time the 16 different sorts, the standard Python library module \texttt{timeit} is used. This module is especially useful as it allows us to set up the environment of the interpreter seperately before starting the timer, and resetting it afterwards. This means that we can read the csv file into memory as either integers or floating points \textit{before} invoking the sort method of interest, only timing the sort itself and not the setup process. It is not the cost of initializing floats vs. integers that interest us, but rather the cost of handling them when they have been initialized.

The timeit module also allows us to repeat the sort one hundred times, only saving the fastest of the runtimes. Why do we only save the fastest runtime, when we could have calculated the mean and standard deviation of all the runtimes? It is considered best practice to only analyze the fastest of the runtimes when profiling the efficiency of an algorithm, given constant input. This is due to the runtime of an algorithm being easily influenced by other processes on the operating system. To give a somewhat arbitrary example, if your email client suddenly decides to fetch all your new emails during the run of the algorithm, the runtime of that one run becomes exceptionally large, and not indicative of the efficiency and optimality of the algorithm itself. The minimum of several runs though \textit{is} indicative of the efficiency of the algorithm. Therefore we have chosen the minimum of the runtimes to be the response variable of the experiment.

The resulting runtime is given in seconds as a floating point number with 15 decimal points of accuracy. This is for all intents and purposes enough numerical accuracy, as for the reasons explained above other factors will add more than $10^{-15}$ seconds of noise to the response data. We still believe that the repetitions of the experiments and picking the fastest runtime will reduce this noise to a level where analysis is feasible.
