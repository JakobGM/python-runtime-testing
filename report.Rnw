\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{listings}
\usepackage{bm} % For bold vector symbols with \bm

\renewcommand{\thesection}{\arabic{section}.}

% \newcommand\given[1][]{\:#1\vert\:}
% \newcommand{\N}{\mathbb{N}}
%\newcommand{\E}{\mathrm{E}}
% \newcommand{\P}{\mathrm{P}}
% \newcommand{\Var}{\mathrm{Var}}
% \newcommand{\Cov}{\mathrm{Cov}}
% \newcommand{\me}{\mathrm{e}}

\title{TMA4267 Design of Experiment}
\author{Øyvind Klåpbakken, Jakob Martinussen}
\date{Spring 2017}

\begin{document}

\maketitle
\tableofcontents

<<echo=FALSE,message=FALSE,warning=FALSE>>=
read_chunk("R-code/R_for_latex.R")
@

<<setup,echo=FALSE,hide=TRUE,warning=FALSE,message=FALSE>>=
options(width=50)
@

\section{Introduction}

\input{"latex/introduction.tex"}

\section{Experiment}

\input{"latex/experiment.tex"}

\section{Analysis}

A full factorial $2^k$ experiment with $k = 4$ factors is conducted, with the order of the individual runs randomly generated by \texttt{FrF2}. The number of individual runs is $n = 16$. The number of covariates, $p$, will depend on how many of the interaction effects one wishes to include as covariates in the full model. Replicates of the individual runs are not necessary due to the way the response is recorded during the experiment, that is, the minimum runtime of several runs is recorded as the response. Blocking is not necessary since all the experiments are done on the same computer during one continuous session without any change to hardware or software. 

The first step of the analysis involves fitting the full model. This produces estimates for $\bm{\beta}$, but the standard error of the individual ${\beta}_k,\ k = 1,\ 2,\ ...,\ p$ can not be computed due to the degrees of freedom, $n - p$, being zero. Because of this the significant covariates can not be found in the traditional way. An alternative approach must therefore be taken. Lenth introduced a method for estimating which covariates are significant. This method is implemented in the code, see the function \texttt{get\_lenth} in the appendix. The function returns a list of significant effects for a given significance level, as well as a Pareto plot showing the same thing visually. Lenth's approach reveals that the four factor interaction isn't significant for any reasonable significance level. This can be seen in figure \ref{fig:full_model}. This, in addition to the unclear interpretation of such an interaction, leads us to drop the four factor interaction from the model in order to obtain a degree of freedom and a simpler model.

<<full_model, fig.cap="Estimation of significant covariates. The straigt vertical line represents the minmum threshold required for the covariate to be considered significant", fig.height=4>>=
@

Fitting a new, reduced model without the four factor interaction reveals that none of the three factor interactions are significant either. This can seen by calling \texttt{summary(lm3)}. The code is included in the appendix. In addition, the F-statistic reveals that the regression isn't significant for this model. One can therefore proceed with a model with only one - and two - factor interactions. This has the benefit of enabling us to make plots of the studentized residuals in order to evaluate the correctness of the assumptions made when constructing a linear model. This was not possible for the earlier models. 

Before one proceeds to constructing the final model it is sensible to consider how a transformation of the response could impact the fit of the model. A widely used transformation is the Box-Cox transformation. The value that maximizes the log-likelihood function calculated by \texttt{boxcox} is chosen as $\lambda$ in the transformation given by equation \eqref{boxcox}. $T_{new}$ denotes the new response. $T$ denotes the original response.

\begin{equation}\label{boxcox}
T_{new} = \frac{T^{\lambda} - 1}{\lambda}
\end{equation}

<<reduced_model_2_tf>>=
@

The transformation is carried out in the code. Lambda is found by the use of \texttt{get{\_}lambda}. A new model with the transformed response is then fitted. Calling \texttt{summary} on the model, as done in the code, returns a overview of some of the most important properties of the model. The model constructed from the transformed data has, among other things, a slightly higher value of $R^2$ than in the untransformed case. This suggests that model now describes more of the variability in the data. By looking at the F-statistic and it's associated p-value one observes that the regression is significant with significance level $\alpha = 0.05$, which wasn't the case for the model that included three factor interactions.

By the lookig at the output from calling \texttt{summary} on the transformed model one observes that not all of the covariates are significant. The only covariates that are significant at a significance level of $\alpha \leq 0.05$ are A, D, the interaction between A and B, A and D and C and D. 

This, in more plain language, means that the way two variables are interchanged probably has an effect on the running time of the function. Other factors that seem to make a difference is the way data is represented, aswell as the interaction between variable reassignment and list generation, variable reassignment and data representation, aswell as the interaction between the method used for returning variables and data representation.

<<analysis>>=
@


<<effects, fig.cap=c("Estimation of main effects", "Estimated change in response when changing the level of an individual factor"), fig.height=3, fig.width=4>>=
@

The main effects are shown in figure \ref{fig:effects1}. This shows that data representation and variable resassignment affects the response by a considerable amount.  The plot shows that other factors are also estimated to have an effect, but these effects are not significant and can not be trusted to reflect the truth. Extracting reliable information from the interaction plot, shown in \ref{fig:effects2} isn't quite as straigth-forward. The panel corresponding to the interaction between A and D probably reflects the truth, but the other plots are impacted by inclusion of covariates that aren't significant and shouldn't be considered to be accurate.

<<residuals, fig.cap="Left panel displays studentized residuals plotted against the recorded response. Right panel shows the sample quantiles plotted against the theoretical normal distribution", fig.height=3>>=
@

The model assumptions for the linear regression is that the errors have homoscedastic variance and are normally distributed with mean zero. There must be linearity of covariates, uncorrelated errors and additivity of errors. The quantile plot, seen in \ref{fig:residuals}, suggests that the errors are normally distributed. The studentized residuals does, however, seem to be increasing as the response grows. This contradicts our assumptions and suggests that the linear model used isn't a true reflection of the actual model. The doesn't necessarily mean that our model is bad choice, but rather that there are effects that hasn't been accounted for.

\section{Conclusion}

\input{"latex/conclusion.tex"}

\end{document}
