\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{listings}

\renewcommand{\thesection}{\arabic{section}.}

% \newcommand\given[1][]{\:#1\vert\:}
% \newcommand{\N}{\mathbb{N}}
%\newcommand{\E}{\mathrm{E}}
% \newcommand{\P}{\mathrm{P}}
% \newcommand{\Var}{\mathrm{Var}}
% \newcommand{\Cov}{\mathrm{Cov}}
% \newcommand{\me}{\mathrm{e}}

\title{TMA4267 Design of Experiment}
\author{Øyvind Klåpbakken, Jakob Martinussen}
\date{Spring 2017}

\begin{document}

\maketitle
\tableofcontents

<<echo=FALSE,message=FALSE,warning=FALSE>>=
read_chunk("R-code/R_for_latex.R")
@

<<setup,echo=FALSE,hide=TRUE,warning=FALSE,message=FALSE>>=
options(width=50)
@

\section{Introduction}

\input{"latex/introduction.tex"}

\section{Experiment}

\input{"latex/experiment.tex"}

\section{}

\section{Analysis}

A full factorial $2^k$ experiment with $k = 4$ factors is conducted, with the order of the individual runs randomly generated by \texttt{FrF2}. The number of individual runs is $n = 16$. The number of covariates, $p$, will depend on how many of the interaction effects one wishes to include as covariates. Replicates of the individual runs is not necessary due to the way the response is found. The response is already the smallest time recorded when calling a function 100 times in a row. The probability that the response has taken on an very unusual value is therefore already quite low. Each run is not necessarily an genuine run replicate since not all processes taking up resources on the computer and be monitored and controlled. Despite this the response is not expected to have been significantly impacted by this. This is due the fact that the function is called several times before the response is recorded, and the processes would probably not run for the full duration of the test. Blocking is not necessary since all the experiments is done on the same computer during one continuous session without any change to hardware or software. 

The first step of the analysis involves fitting the full model. This produces estimates for $\mathbf{\beta}$, but the standard error of the individual ${\beta}_k, k = 1,2,...,p$ can not be computed due to the degrees of freedom, $n - p$, being zero. Because of this the significant covariates can not be found in the traditional way. An alternative approach must therefore be taken. Lenth provided a method for estimating which covariates are significant. This method is implemented in the code, see the function \texttt{get\_lenth}. Lenth's approach reveals that the four factor interaction isn't significant for any reasonable significance level. This can be seen in figure ref{fig:full_model}. This, in addition to the unclear meaning of such an interaction leads us to drop the four factor interaction from the model in order to obtain a degree of freedom. 

<<full_model, fig.cap="Estimation of significant covariates", fig.height=4>>=
@

Fitting a new, reduced model without the four factor interaction reveals that none of three factor interactions are significant either. This can seen by calling \texttt{summary(lm3)}. In addition, the F-statistic reveals that the regression isn't significant for this model. One can therefore proceed with a model with only one - and two - factor interactions. This has the benefit of enabling us to make plots of the residuals in order to evaluate the correctness of the assumptions made when constructing a linear model. 


Before one proceeds to constructing the final model it is sensible to consider how a transformation of the response could impact the fit of the model. A widely used transformation is the Box-Cox transformation. The value that maximizes the log-likelihood function calculated by \texttt{boxcox} is chosen as $\lambda$ in the transformation given by equation \eqref{boxcox}. $T_{new}$ denotes the new response. $T$ denotes the original response.

\begin{equation}\label{boxcox}
T_{new} = \frac{T^{\lambda} - 1}{\lambda}
\end{equation}

<<reduced_model_2_tf>>=
@

The transformation is carried out in the code. Lambda is found by the use of \texttt{get{\_}lambda}. A new model with the transformed response is then fitted. Calling \texttt{summary} on the model, as done in the code, returns a overview of some of the most important properties of the model. The model constructed from the transformed data has, among other things, a slightly higher value of $R^2$ than in the untransformed case. This suggests that model now describes more of the variability in the data. By looking at the F-statistic and it's associated p-value one observes that the regression is significant with significance level $\alpha = 0.05$, which wasn't the case for the model that included three factor interactions.

By the lookig at the output from calling \texttt{summary} on the transformed model one observes that not all of the covariates are significant. The only covariates that are significant at a significance level of $\alpha \leq 0.05$ are A, D, the interaction between A and B, A and D and C and D. 

This, in more plain language, means that the way two variables are reassigned probably has an effect on the running time of the function. Other factors that seem to make a difference is the way data is represented, aswell as the interaction between variable reassignment and list generation, variable reassignment and data representation, aswell as the interaction between the method used for returning variables and data representation. 

<<analysis>>=
@


<<effects, fig.height=3, fig.width=4>>=
@

The main effects are shown in figure \ref{fig:effects}. This shows that data representation and variable resassignment affects the response.  The plot also shows that the other factors are also estimated to have an effect, but these effects are not significant and can not be trusted to reflect the truth.

<<residuals, fig.height=3>>=
@


<<quantile,fig.height=3>>=
@


The model assumptions for the linear regression is that the errors have homoscedastic variance and are normally distributed with mean zero. There must be linearity of covariates, uncorrelated errors and additivity of errors. The quantile plot suggests that the errors are normally distributed. The studentized residuals does, however, seem to be increasing as the response grows. This contradicts our assumptions and suggests that the linear model used isn't a true reflection of the actual model. 

\section{Conclusion}

\input{"latex/conclusion.tex"}

\end{document}