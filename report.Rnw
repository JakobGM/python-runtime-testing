\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{listings}
\usepackage{float}
\usepackage[margin=1.7in]{geometry}
\usepackage{bm} % For bold vector symbols with \bm

\renewcommand{\thesection}{\arabic{section}.}

% \newcommand\given[1][]{\:#1\vert\:}
% \newcommand{\N}{\mathbb{N}}
%\newcommand{\E}{\mathrm{E}}
% \newcommand{\P}{\mathrm{P}}
% \newcommand{\Var}{\mathrm{Var}}
% \newcommand{\Cov}{\mathrm{Cov}}
% \newcommand{\me}{\mathrm{e}}

\title{TMA4267 Design of Experiment}
\author{Øyvind Klåpbakken, Jakob Gerhard Martinussen}
\date{Spring 2017}

\begin{document}

\maketitle

<<echo=FALSE,message=FALSE,warning=FALSE>>=
read_chunk("R-code/R_for_latex.R")
@

<<setup,echo=FALSE,hide=TRUE,warning=FALSE,message=FALSE>>=
options(width=50)
@

\section{Introduction}

\input{"latex/introduction.tex"}

\section{Experiment}

\input{"latex/experiment.tex"}

\section{Analysis}

A full factorial $2^k$ experiment with $k = 4$ factors is conducted, with the order of the individual runs randomly generated by \texttt{FrF2}. The number of individual runs is $n = 16$. The number of covariates, $p$, will depend on how many of the interaction effects one wishes to include as covariates in the full model. Replicates of the individual runs are not necessary due to the way the response is recorded during the experiment, that is, the minimum runtime of several runs is recorded as the response. Blocking is not necessary since all the experiments are done on the same computer during one continuous session without any change to hardware or software. 

The first step of the analysis involves fitting the full model. This produces estimates for $\bm{\beta}$, but the standard error of the individual ${\beta}_k,\ k = 1,\ 2,\ ...,\ p$ can not be computed due to the degrees of freedom, $n - p$, being zero. Because of this the significant covariates can not be found in the traditional way. An alternative approach must therefore be taken. Lenth introduced a method for estimating which covariates are significant. This method is implemented in the code, see the function \texttt{get\_lenth} in the appendix. The function returns a list of significant effects for a given significance level, as well as a Pareto plot showing the same thing visually. Lenth's approach reveals that the four factor interaction isn't significant for any reasonable significance level. This can be seen in figure \ref{fig:full_model}. This, in addition to the unclear interpretation of such an interaction, leads us to drop the four factor interaction from the model in order to obtain a degree of freedom and a simpler model.

<<full_model, fig.cap="Estimation of significant covariates. The straigt vertical line represents the minmum threshold required for the covariate to be considered significant", fig.height=4>>=
@

Fitting a new, reduced model without the four factor interaction reveals that none of the three factor interactions are significant either. This can seen by calling \texttt{summary(lm3)}. The code is included in the appendix. In addition, the F-statistic reveals that the regression isn't significant for this model. One can therefore proceed with a model with only main effects and two-factor interactions. This has the benefit of enabling us to make plots of the studentized residuals in order to evaluate the correctness of the assumptions made when constructing a linear model. 

Before one proceeds to constructing the final model it is sensible to consider how a transformation of the response could impact the model. A widely used transformation is the Box-Cox transformation. This transformation is done in order to increase the normality of residuals. The value that maximizes the \texttt{boxcox} log-likelihood function is denoted as $\lambda$. The associated transformation is given by equation \eqref{boxcox}. $T_{new}$ denotes the new response, $T$ denotes the original response.

\begin{equation}\label{boxcox}
T_{new} = \frac{T^{\lambda} - 1}{\lambda}
\end{equation}

<<reduced_model_2_tf>>=
@

The transformation is implemented in the code. Lambda is found by the use of \texttt{get{\_}lambda}. A new model with the transformed response is then fitted. Calling \texttt{summary} on the model, as done in the code, returns an overview of some of the most important properties of the model. The model constructed from the transformed data has a slightly higher value for $R^2$ than in the non-transformed case. The model now describes more of the variability in the data. By looking at the printed F-statistic and it's associated p-value one observes that the regression is significant with significance level $\alpha = 0.05$, which wasn't the case for the model that included three factor interactions.

By the looking at the output from calling \texttt{summary} on the transformed model one observes that not all of the covariates are significant. The only covariates that are significant at a significance level of $\alpha \leq 0.05$ are $A$, $D$, the interaction between $A$ and $B$, $A$ and $D$ and $C$ and $D$. 

In other words, this means that the way two variables are interchanged probably has an effect on the running time of the function, and likewise for which data type is used for the numbers. Regarding significant interaction effects, we have the interaction between variable reassignment and list generation, variable reassignment and data representation, as well as between the method used for returning variables and data representation.

A very clear interaction is observed betweeen variable reassignment and data representation. By looking at panel $(1,4)$ in figure \ref{fig:effects2} one can observe the nature of this interaction in detail. When variable reassignment is done by use of tuple reassignment one observes that the running time increases by a considerable amount when data is represented as floating points instead of integers. The panel also shows that when a temporary variable is used (i.e. $A=-1$), the choice of data type is almost negligible.

<<analysis>>=
@


<<effects, fig.cap=c("Estimation of main effects", "Estimated change in response when changing the level of an individual factor"), fig.height=3, fig.pos="h">>=
@

The main effects are shown in figure \ref{fig:effects1}. This shows that data representation and variable reassignment affects the response by a considerable amount.  The plot shows that other factors are also estimated to have an effect, but these effects are not significant and can not be trusted to reflect the truth. 

<<residuals, fig.cap="Left panel displays studentized residuals plotted against the recorded response. Right panel shows the sample quantiles plotted against the theoretical normal distribution", fig.height=3, fig.pos="h">>=
@

The model assumptions for the normal linear regression is that the errors have homoscedastic variance and are normally distributed with mean zero. There must be linear relation between the covariates and the response, in addition to uncorrelated errors and additivity of errors. The quantile plot, seen in \ref{fig:residuals}, suggests that the errors are normally distributed. The studentized residuals does, however, seem to be increasing as the response increases. This weakens some of our assumptions and suggests that the linear model might not be a perfect reflection of reality.

\section{Conclusion}

\input{"latex/conclusion.tex"}

\end{document}
